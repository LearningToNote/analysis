SET SCHEMA LTN_DEVELOP;

DROP PROCEDURE DOWNSAMPLE_R;
CREATE PROCEDURE DOWNSAMPLE_R(IN data T_TRAIN_DATA_DOCS, OUT train_data T_TRAIN_DATA)
LANGUAGE RLANG AS
BEGIN
	set.seed(as.integer(format(Sys.time(), "%s")))

	#### down sampling
	true_pairs <- data[data$DDI != -1,-1]
	false_pairs <- data[data$DDI == -1,-1]
	if (nrow(true_pairs) > 0) {
		false_downsampled_index <- sample(1:nrow(false_pairs), min(nrow(false_pairs), nrow(true_pairs)*1.5) )
		false_downsampled <- false_pairs[false_downsampled_index,]
		train_data <- rbind(true_pairs, false_downsampled)
	} else {
		train_data <- false_pairs
	}

END;

DROP PROCEDURE CREATE_TRAINING_DATA;
CREATE PROCEDURE CREATE_TRAINING_DATA(IN task_id INT, OUT train_data T_TRAIN_DATA)
LANGUAGE SQLSCRIPT AS
BEGIN
DECLARE FTI LTN_DEVELOP.T_INDEX;

CALL get_fulltext_index_for_task(:task_id, :FTI);

unsampled_data =
	SELECT DOC_ID, DDI, E1_ID, E2_ID, E1_TYPE, E2_TYPE, CHAR_DIST, WORD_DIST,
	REPLACE_REGEXPR('.* (\w+[ \:\,\;]* \w+[ \:\,\;]* \w+[ \:\,\;]*)$' IN STRING_AGG(T1, ' ') WITH '\1' OCCURRENCE ALL) AS "BEFORE",
	REPLACE_REGEXPR('^([\:\,\; ]*\w+[ \:\,\;]* \w+[ \:\,\;]* \w+[ \:\,\;]*).* (\w+[ \:\,\;]* \w+[ \:\,\;]* \w+[ \:\,\;]*)$' IN STRING_AGG(T2, ' ') WITH '\1 \2' OCCURRENCE ALL) AS BETWEEN,
	REPLACE_REGEXPR('^([\:\,\; ]*\w+[ \:\,\;]* \w+[ \:\,\;]* \w+[ \:\,\;]*).*' IN STRING_AGG(T3, ' ') WITH '\1' OCCURRENCE ALL) AS AFTER,
	REPLACE_REGEXPR('.* (\w+ \w+ \w+)$' IN STRING_AGG(P1, ' ') WITH '\1' OCCURRENCE ALL) AS P_BEFORE,
	REPLACE_REGEXPR('^(\w+ \w+ \w+).* (\w+ \w+ \w+)$' IN STRING_AGG(P2, ' ') WITH '\1 \2' OCCURRENCE ALL) AS P_BETWEEN,
	REPLACE_REGEXPR('^(\w+ \w+ \w+).*' IN STRING_AGG(P3, ' ') WITH '\1' OCCURRENCE ALL) AS P_AFTER
	FROM (
		SELECT DOC_ID, E1_ID, E2_ID, DDI, E1_TYPE, E2_TYPE, CHAR_DIST, WORD_DIST,
		STRING_AGG(T1, ' ' ORDER BY COUNTER) AS T1,
		STRING_AGG(T2, ' ' ORDER BY COUNTER) AS T2,
		STRING_AGG(T3, ' ' ORDER BY COUNTER) AS T3,
		STRING_AGG(P1, ' ' ORDER BY COUNTER) AS P1,
		STRING_AGG(P2, ' ' ORDER BY COUNTER) AS P2,
		STRING_AGG(P3, ' ' ORDER BY COUNTER) AS P3
		FROM(
			SELECT DOC_ID, E1_ID, E2_ID, DDI, E1_TYPE, E2_TYPE, CHAR_DIST, WORD_DIST,
			CASE WHEN POSITION < 0 THEN TOKEN ELSE NULL END AS T1,
			CASE WHEN POSITION = 0 THEN TOKEN ELSE NULL END AS T2,
			CASE WHEN POSITION > 0 THEN TOKEN ELSE NULL END AS T3,

			CASE WHEN POSITION < 0 THEN POS ELSE NULL END AS P1,
			CASE WHEN POSITION = 0 THEN POS ELSE NULL END AS P2,
			CASE WHEN POSITION > 0 THEN POS ELSE NULL END AS P3,

			COUNTER,
			POSITION
			FROM (
				SELECT
				UD.ID AS DOC_ID,
				E1_ID,
				E2_ID,
				CASE WHEN P.TYPE_ID IS NULL THEN -1 ELSE P.TYPE_ID END AS DDI,
				E1.TYPE_ID AS E1_TYPE,
				E2.TYPE_ID AS E2_TYPE,
				O2."START" - O1."END" AS CHAR_DIST,
				FTI2.TA_COUNTER - FTI1.TA_COUNTER AS WORD_DIST,
				CASE
	                WHEN FTI.TA_NORMALIZED IS NOT NULL THEN FTI.TA_NORMALIZED
	                WHEN FTI.TA_STEM IS NOT NULL THEN FTI.TA_STEM
	                ELSE FTI.TA_TOKEN
	            END AS TOKEN,
				POSTAGS.ID as POS,
				CASE
				    WHEN FTI.TA_COUNTER < FTI1.TA_COUNTER THEN -1
				    WHEN (FTI.TA_COUNTER > FTI1.TA_COUNTER AND FTI.TA_COUNTER < FTI2.TA_COUNTER) THEN 0
				    WHEN FTI.TA_COUNTER > FTI2.TA_COUNTER THEN 1
				END AS POSITION,
				FTI.TA_COUNTER AS COUNTER
				FROM LEARNING_TO_NOTE.PAIRS P
				JOIN LEARNING_TO_NOTE.ENTITIES E1 ON P.E1_ID = E1.ID
				JOIN LEARNING_TO_NOTE.ENTITIES E2 ON P.E2_ID = E2.ID
				JOIN LEARNING_TO_NOTE.USER_DOCUMENTS UD ON E1.USER_DOC_ID = UD.ID AND E2.USER_DOC_ID = UD.ID AND P.USER_DOC_ID = UD.ID
				JOIN LEARNING_TO_NOTE.OFFSETS O1 ON O1.ENTITY_ID = E1.ID AND O1.USER_DOC_ID=UD.ID
				JOIN LEARNING_TO_NOTE.OFFSETS O2 ON O2.ENTITY_ID = E2.ID AND O2.USER_DOC_ID=UD.ID
				JOIN :FTI FTI1 ON FTI1.DOCUMENT_ID = UD.DOCUMENT_ID AND FTI1.TA_OFFSET = O1."START"
				JOIN :FTI FTI2 ON FTI2.DOCUMENT_ID = UD.DOCUMENT_ID AND FTI2.TA_OFFSET = O2."START"
				JOIN :FTI FTI ON FTI.DOCUMENT_ID = UD.DOCUMENT_ID
				JOIN LEARNING_TO_NOTE.POS_TAGS POSTAGS ON FTI.TA_TYPE = POSTAGS.POS
				WHERE UD.USER_ID = 'DDI-IMPORTER'
				AND (FTI.TA_TYPE <> 'punctuation' OR FTI.TA_TOKEN = ':' OR FTI.TA_TOKEN = ',' OR FTI.TA_TOKEN = ';')
				AND FTI.TA_TYPE <> 'number'
				AND FTI.TA_TOKEN NOT LIKE '%!%%' ESCAPE '!'

				AND FTI1.TA_SENTENCE = FTI2.TA_SENTENCE
				AND FTI.TA_SENTENCE = FTI1.TA_SENTENCE
				AND FTI.TA_COUNTER <> FTI1.TA_COUNTER
				AND FTI.TA_COUNTER <> FTI2.TA_COUNTER
				AND FTI1.TA_COUNTER < FTI2.TA_COUNTER

				ORDER BY FTI.TA_COUNTER
			)
			WHERE TOKEN NOT IN (SELECT STOPWORD FROM LEARNING_TO_NOTE.STOPWORDS)
		)
		GROUP BY DOC_ID, E1_ID, E2_ID, DDI, E1_TYPE, E2_TYPE, CHAR_DIST, WORD_DIST, POSITION
		ORDER BY DOC_ID, E1_ID, E2_ID, POSITION
	)
	GROUP BY DOC_ID, E1_ID, E2_ID, DDI, E1_TYPE, E2_TYPE, CHAR_DIST, WORD_DIST;

CALL DOWNSAMPLE_R(:unsampled_data, :train_data);

END;


CALL CREATE_TRAINING_DATA(1, ?);

